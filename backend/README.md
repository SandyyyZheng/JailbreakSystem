# ğŸ”½ Backend

This is the backend documentation for the Jailbreak System.

## Setup

1. Create a virtual environment:
```bash
cd backend
python -m venv venv
source venv/bin/activate  # On Windows: venv/Scripts/activate
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Set up environment variables:
```bash
cp .env.example .env
```
Then edit the `.env` file with your API keys and configuration.

## Running the Server

Development mode:
```bash
python app.py
```

The server will start on http://localhost:5001

## Project Structure

```
backend/
â”œâ”€â”€ app.py                # Main application entry point
â”œâ”€â”€ models/               # Database models
â”œâ”€â”€ controllers/          # API controllers
â”œâ”€â”€ utils/                # Utility functions
â”‚   â”œâ”€â”€ jailbreak_algorithms.py # Jailbreak implementation
â”‚   â””â”€â”€ llm_api.py        # LLM API integration
â”œâ”€â”€ database/             # Database files
â”‚   â”œâ”€â”€ jailbreak.db      # SQLite database
â”‚   â””â”€â”€ db_setup.py       # Database initialization
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ .env.example          # Environment variables template
```

## API Endpoints

### Attacks

- `GET /api/attacks/` - Get all attacks
- `GET /api/attacks/<id>` - Get a specific attack
- `POST /api/attacks/` - Create a new attack
- `PUT /api/attacks/<id>` - Update an attack
- `DELETE /api/attacks/<id>` - Delete an attack
- `POST /api/attacks/execute` - Execute an attack on a prompt

### Prompts

- `GET /api/prompts/` - Get all prompts
- `GET /api/prompts/<id>` - Get a specific prompt
- `POST /api/prompts/` - Create a new prompt
- `PUT /api/prompts/<id>` - Update a prompt
- `DELETE /api/prompts/<id>` - Delete a prompt
- `GET /api/prompts/categories` - Get all prompt categories

### Results

- `GET /api/results/` - Get all results
- `GET /api/results/<id>` - Get a specific result
- `POST /api/results/` - Create a new result
- `PUT /api/results/<id>` - Update a result
- `DELETE /api/results/<id>` - Delete a result
- `GET /api/results/stats` - Get statistics about jailbreak success rates

## Implementing Custom Jailbreak Algorithms

To implement your own black-box attack algorithms:

1. Navigate to `utils/jailbreak_algorithms.py`
2. Add your custom function:

```python
def my_custom_jailbreak(prompt, param1="value1", param2="value2"):
    # Your implementation here
    return modified_prompt

# Then add it to the algorithms dictionary
algorithms["my_custom_algorithm"] = my_custom_jailbreak
```

## Database

The system uses SQLite for development. The database file is located at `database/jailbreak.db`.

## Technology Stack

- Python 3.8+
- Flask for Web framework
- SQLite for Database
- LLM API Integrations:
  - OpenAI API (GPT)
  - Anthropic API (Claude) 

## ğŸªº Easter Egg
- å¹¶éå½©è›‹ï¼Œç”Ÿæ´»æ‰€è¿«ã€‚
- ç”±äºHFUTæ¯•è®¾å­˜åœ¨ç°åœºéªŒæ”¶ç¯èŠ‚ï¼Œé‚åŠ å…¥äº†ä¸€äº›å¿…è¦çš„ã€Œä¿é™©æªæ–½ã€ï¼Œä»¥ä¿è¯è¿è¡Œæ—¶ä¸ä¼šå‡ºç°â€œå…¨å†›è¦†æ²¡â€çš„æƒ…å†µã€‚å¦‚éœ€å…³æ‰ã€Œä¿é™©æªæ–½ã€ï¼Œè¯·ç›´æ¥å°†[config.py](utils/config.py)ä¸­çš„PROBABILITYå‚æ•°è°ƒä¸º0å³å¯.